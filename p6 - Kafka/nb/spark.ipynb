{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c893dd-5fef-4dd0-9876-8c36424b5c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3: Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7769d22a-d364-4811-8224-43aef9aee1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import col, expr, row_number, from_json\n",
    "from pyspark.sql.functions import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0323be5-4602-4071-afac-34ecbc9f90ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3e960f7e-8b8f-49f3-a5ba-91c8f2b9e7a3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.2.2/spark-sql-kafka-0-10_2.12-3.2.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.2!spark-sql-kafka-0-10_2.12.jar (68ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.2.2/spark-token-provider-kafka-0-10_2.12-3.2.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.2!spark-token-provider-kafka-0-10_2.12.jar (18ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.8.1/kafka-clients-2.8.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.8.1!kafka-clients.jar (266ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (15ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.6.2/commons-pool2-2.6.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.6.2!commons-pool2.jar (25ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (17ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.1/hadoop-client-runtime-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.1!hadoop-client-runtime.jar (624ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.7.1!lz4-java.jar (21ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.4!snappy-java.jar(bundle) (34ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (15ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.1/hadoop-client-api-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.1!hadoop-client-api.jar (248ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.htrace#htrace-core4;4.1.0-incubating!htrace-core4.jar (29ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (15ms)\n",
      ":: resolution report :: resolve 7120ms :: artifacts dl 1420ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   13  |   13  |   0   ||   13  |   13  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3e960f7e-8b8f-49f3-a5ba-91c8f2b9e7a3\n",
      "\tconfs: [default]\n",
      "\t13 artifacts copied, 0 already retrieved (59188kB/169ms)\n",
      "23/04/28 21:01:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", 10)\n",
    "         .config(\"spark.ui.showConsoleProgress\", False)\n",
    "         .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.2')\n",
    "         .getOrCreate())\n",
    "\n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"stations-json\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e35feb4-2a83-45a0-987c-1167daff03d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80f5e3bb-1469-4957-b3d9-ae53a75ba80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"station STRING, date DATE, degrees DOUBLE, raining INTEGER\"\n",
    "stations_json = (df.select(\n",
    "    col(\"key\").cast(\"string\"), \n",
    "    from_json(col(\"value\").cast(\"string\"), schema).alias(\"value\")).select(\"key\", \"value.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c06a661-f0e0-4c36-a60d-962ba755f5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, station: string, date: date, degrees: double, raining: int]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4c856b1-becf-4787-9fdd-a86f60f0f1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_json.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9cc38ba-ec9e-4888-847f-99c68c9efef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f107659-77ea-480e-b4a0-6a6666f35f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/28 21:02:01 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-da81244b-af47-46cf-bf39-388097d664fa. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/28 21:02:01 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|station|     start|       end|measurements|          avg_temp|         max_temp|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|      A|2000-01-01|2000-04-19|         110| 32.80299305471417|68.03134015480184|\n",
      "|      B|2000-01-01|2000-04-19|         110| 42.40158052349588|78.68175239288585|\n",
      "|      C|2000-01-01|2000-04-19|         110| 40.89114937158351|67.22620383475424|\n",
      "|      D|2000-01-01|2000-04-19|         110| 19.80893376198706|56.67195165651097|\n",
      "|      E|2000-01-01|2000-04-19|         110|27.109123091357272|61.88725386538411|\n",
      "|      F|2000-01-01|2000-04-19|         110| 39.70651580674823| 74.3620554541848|\n",
      "|      G|2000-01-01|2000-04-19|         110| 30.83161194114009| 67.7774949347819|\n",
      "|      H|2000-01-01|2000-04-19|         110| 36.76185850294383|60.30637273006978|\n",
      "|      I|2000-01-01|2000-04-19|         110| 47.72842044783244|86.78582063182435|\n",
      "|      J|2000-01-01|2000-04-19|         110| 37.28103185217006|71.06940819312499|\n",
      "|      K|2000-01-01|2000-04-19|         110| 32.81293811052641|66.88522999831665|\n",
      "|      L|2000-01-01|2000-04-19|         110|25.462684697192174|63.89364450337359|\n",
      "|      M|2000-01-01|2000-04-19|         110|51.327310289589235|78.66452588652153|\n",
      "|      N|2000-01-01|2000-04-19|         110| 49.09364165673738|92.13053133111676|\n",
      "|      O|2000-01-01|2000-04-19|         110| 47.33916984689459|77.95988111046212|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/28 21:02:18 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 16551 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|station|     start|       end|measurements|          avg_temp|         max_temp|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|      A|2000-01-01|2000-04-30|         121| 35.82021717566553|80.31178172940585|\n",
      "|      B|2000-01-01|2000-04-30|         121| 44.10600713637261|78.68175239288585|\n",
      "|      C|2000-01-01|2000-04-30|         121| 43.29001425950536|80.21680463474156|\n",
      "|      D|2000-01-01|2000-04-30|         121| 22.24311688414182|56.67195165651097|\n",
      "|      E|2000-01-01|2000-04-30|         121| 28.34007469332162|61.88725386538411|\n",
      "|      F|2000-01-01|2000-04-30|         121| 41.92348786370325|76.31898672762796|\n",
      "|      G|2000-01-01|2000-04-30|         121|32.434060816037054| 67.7774949347819|\n",
      "|      H|2000-01-01|2000-04-30|         121|39.552857610911374|74.70794418018109|\n",
      "|      I|2000-01-01|2000-04-30|         121| 50.27174836695174|86.78582063182435|\n",
      "|      J|2000-01-01|2000-04-30|         121| 40.14567662823412|78.94389577032383|\n",
      "|      K|2000-01-01|2000-04-30|         121| 34.98730175710674|   67.26148987021|\n",
      "|      L|2000-01-01|2000-04-30|         121|27.897289049721373|63.89364450337359|\n",
      "|      M|2000-01-01|2000-04-30|         121|53.154944630180374|91.18934513768315|\n",
      "|      N|2000-01-01|2000-04-30|         121| 50.98420384890591|92.13053133111676|\n",
      "|      O|2000-01-01|2000-04-30|         121| 48.65169904413878|77.95988111046212|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|station|     start|       end|measurements|          avg_temp|         max_temp|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|      A|2000-01-01|2000-05-04|         125| 37.04331453434791|80.31178172940585|\n",
      "|      B|2000-01-01|2000-05-04|         125| 44.78353408589731|78.68175239288585|\n",
      "|      C|2000-01-01|2000-05-04|         125| 44.04102186551234|80.21680463474156|\n",
      "|      D|2000-01-01|2000-05-04|         125|23.408664745599452| 63.8323765379197|\n",
      "|      E|2000-01-01|2000-05-04|         125|29.286972399889653|66.23745602243716|\n",
      "|      F|2000-01-01|2000-05-04|         125|42.482268807317375|76.31898672762796|\n",
      "|      G|2000-01-01|2000-05-04|         125| 33.04811090479705| 67.7774949347819|\n",
      "|      H|2000-01-01|2000-05-04|         125|40.732346569858464|79.76375295817903|\n",
      "|      I|2000-01-01|2000-05-04|         125| 51.12614346513592|86.78582063182435|\n",
      "|      J|2000-01-01|2000-05-04|         125|40.914880277507024|78.94389577032383|\n",
      "|      K|2000-01-01|2000-05-04|         125|35.699958988124564|   67.26148987021|\n",
      "|      L|2000-01-01|2000-05-04|         125|28.827131113441855|63.89364450337359|\n",
      "|      M|2000-01-01|2000-05-04|         125|54.111626820308274|91.18934513768315|\n",
      "|      N|2000-01-01|2000-05-04|         125| 51.36406850475513|92.13053133111676|\n",
      "|      O|2000-01-01|2000-05-04|         125| 49.25320670499281|77.95988111046212|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|station|     start|       end|measurements|          avg_temp|         max_temp|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|      A|2000-01-01|2000-05-07|         128| 38.09759724606213|86.25459349860921|\n",
      "|      B|2000-01-01|2000-05-07|         128| 44.99195833972162|78.68175239288585|\n",
      "|      C|2000-01-01|2000-05-07|         128|44.579682296709734|80.21680463474156|\n",
      "|      D|2000-01-01|2000-05-07|         128| 24.15488820253606| 63.8323765379197|\n",
      "|      E|2000-01-01|2000-05-07|         128|30.204776283179104| 69.6157504426477|\n",
      "|      F|2000-01-01|2000-05-07|         128| 43.19661463117041|76.31898672762796|\n",
      "|      G|2000-01-01|2000-05-07|         128| 33.89367671607278|72.68214003488112|\n",
      "|      H|2000-01-01|2000-05-07|         128| 41.55681255271663|82.15680103162939|\n",
      "|      I|2000-01-01|2000-05-07|         128| 51.76819581037979|86.78582063182435|\n",
      "|      J|2000-01-01|2000-05-07|         128| 41.21491415605357|78.94389577032383|\n",
      "|      K|2000-01-01|2000-05-07|         128|36.339879326997604|68.67200230832913|\n",
      "|      L|2000-01-01|2000-05-07|         128|29.405505159561397|63.89364450337359|\n",
      "|      M|2000-01-01|2000-05-07|         128|55.033851081826775|99.21508089216309|\n",
      "|      N|2000-01-01|2000-05-07|         128| 51.64513053337877|92.13053133111676|\n",
      "|      O|2000-01-01|2000-05-07|         128| 50.00300484215032| 84.5353876676709|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/28 21:02:31 WARN TaskSetManager: Lost task 4.0 in stage 20.0 (TID 148) (8ae47c622be4 executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/28 21:02:31 WARN TaskSetManager: Lost task 5.0 in stage 20.0 (TID 149) (8ae47c622be4 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "counts_df = (stations_json.groupBy(\"station\").agg(\n",
    "    min(\"date\").alias(\"start\"),\n",
    "    max(\"date\").alias(\"end\"),\n",
    "    count(\"degrees\").alias(\"measurements\"),\n",
    "    avg(\"degrees\").alias(\"avg_temp\"),\n",
    "    max(\"degrees\").alias(\"max_temp\")\n",
    ").orderBy(\"station\"))\n",
    "\n",
    "s = counts_df.writeStream.format(\"console\").trigger(processingTime=\"5 seconds\").outputMode(\"complete\").start()\n",
    "s.awaitTermination(30)\n",
    "s.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2062e12-5561-42e5-82ea-e8eb92e0a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataFrames named today and features that look like this:\n",
    "\n",
    "# DataFrame[station: string, date: date, raining: int]\n",
    "# DataFrame[station: string, date: date, month: int, sub1degrees: float, sub1raining: int, sub2degrees: float, \n",
    "# sub2raining: int]\n",
    "\n",
    "schema = \"station STRING, date DATE, degrees FLOAT, raining INTEGER\"\n",
    "today = (df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"value\"))\n",
    "         .select(\"value.station\", \"value.date\", \"value.raining\")\n",
    "        )\n",
    "\n",
    "\n",
    "sub1features = (df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"value\"))\n",
    "            .select(\"value.station\",\n",
    "                    date_add(\"value.date\", 1).alias(\"date\"),\n",
    "                    col(\"value.degrees\").alias(\"sub1degrees\"),\n",
    "                    col(\"value.raining\").alias(\"sub1raining\")\n",
    "                   )\n",
    "            )\n",
    "sub2features = (df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"value\"))\n",
    "            .select(\"value.station\",\n",
    "                    date_add(\"value.date\", 2).alias(\"date\"),\n",
    "                    col(\"value.degrees\").alias(\"sub2degrees\"),\n",
    "                    col(\"value.raining\").alias(\"sub2raining\")\n",
    "                   )\n",
    "            )\n",
    "\n",
    "features = (df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"value\"))\n",
    "            .select(\"value.station\",\"value.date\", month(\"value.date\").alias(\"month\"))\n",
    "                    .join(sub1features, [\"station\",\"date\"], \"inner\")\n",
    "                    .join(sub2features, [\"station\",\"date\"], \"inner\")\n",
    "                   \n",
    "           )\n",
    "\n",
    "#Join today with features on the date and station columns\n",
    "Rain_Forecast = today.join(features, [\"station\",\"date\"], \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce1d0ac5-476b-4a65-9c88-843664637048",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert str(today) == \"DataFrame[station: string, date: date, raining: int]\"\n",
    "assert str(features) == \"DataFrame[station: string, date: date, month: int, sub1degrees: float, sub1raining: int, sub2degrees: float, sub2raining: int]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc6deec5-639c-4b1e-b39f-73e7877d5d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[station: string, date: date, raining: int, month: int, sub1degrees: float, sub1raining: int, sub2degrees: float, sub2raining: int]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rain_Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a742adb8-46a7-404f-9da7-eee46217cf20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rain_Forecast.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc9ea1d4-844e-4e94-a47c-36d0e63f8220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view final dataframe\n",
    "# s = Rain_Forecast.writeStream.format(\"console\").trigger(processingTime=\"5 seconds\").outputMode(\"append\").start()\n",
    "# s.awaitTermination(30)\n",
    "# s.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68c9a820-fed4-45ca-9f33-e2dd3b59ac4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: `hdfs://nn:9000/Rain_Forecast': No such file or directory\n",
      "`hdfs://nn:9000/Rain_Forecast': Is now a directory\n",
      "cat: `hdfs://nn:9000/Rain_Forecast_Checkpoint': No such file or directory\n",
      "`hdfs://nn:9000/Rain_Forecast_Checkpoint': Is now a directory\n"
     ]
    }
   ],
   "source": [
    "resp = !hdfs dfs -cat hdfs://nn:9000/Rain_Forecast\n",
    "if str(resp[0]) == \"cat: `hdfs://nn:9000/Rain_Forecast': No such file or directory\":\n",
    "    print(resp[0])\n",
    "    !hdfs dfs -mkdir hdfs://nn:9000/Rain_Forecast\n",
    "    resp = !hdfs dfs -cat hdfs://nn:9000/Rain_Forecast\n",
    "    assert resp[0] == \"cat: `hdfs://nn:9000/Rain_Forecast': Is a directory\"\n",
    "    print(\"`hdfs://nn:9000/Rain_Forecast': Is now a directory\")\n",
    "else:\n",
    "    print(\"`hdfs://nn:9000/Rain_Forecast': Is already a directory\")\n",
    "\n",
    "resp = !hdfs dfs -cat hdfs://nn:9000/Rain_Forecast_Checkpoint\n",
    "if resp[0] == \"cat: `hdfs://nn:9000/Rain_Forecast_Checkpoint': No such file or directory\":\n",
    "    print(resp[0])\n",
    "    !hdfs dfs -mkdir hdfs://nn:9000/Rain_Forecast_Checkpoint\n",
    "    resp = !hdfs dfs -cat hdfs://nn:9000/Rain_Forecast_Checkpoint\n",
    "    assert resp[0] == \"cat: `hdfs://nn:9000/Rain_Forecast_Checkpoint': Is a directory\"\n",
    "    print(\"`hdfs://nn:9000/Rain_Forecast_Checkpoint': Is now a directory\")\n",
    "else:\n",
    "    print(\"`hdfs://nn:9000/Rain_Forecast_Checkpoint': Is already a directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bfb0faa-d4d7-4c3f-ba4b-d2ecc70bc765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/28 21:03:14 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "query = (Rain_Forecast\n",
    " .repartition(1)\n",
    " .writeStream\n",
    " .format(\"parquet\")\n",
    " .trigger(processingTime=\"1 minute\")\n",
    " .option(\"path\", \"hdfs://nn:9000/Rain_Forecast\") #HDFS directory that will accumulate parquet files\n",
    " .option(\"checkpointLocation\", \"hdfs://nn:9000/Rain_Forecast_Checkpoint\") #HDFS directory where Spark stores info about how to supress duplicates when reading those parquet files\n",
    " .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e420b65-1de7-4eb7-a733-adf35a20386d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxr-xr-x   - root supergroup          0 2023-04-28 21:04 hdfs://nn:9000/Rain_Forecast/_spark_metadata\n",
      "-rw-r--r--   3 root supergroup      30510 2023-04-28 21:03 hdfs://nn:9000/Rain_Forecast/part-00000-15106b58-5ebb-46dc-af5a-58f52cd6a42e-c000.snappy.parquet\n",
      "-rw-r--r--   3 root supergroup       8727 2023-04-28 21:04 hdfs://nn:9000/Rain_Forecast/part-00000-d8b196a8-03aa-46f2-a224-fbe6cb162676-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "#time.sleep(120)\n",
    "!hdfs dfs -ls hdfs://nn:9000/Rain_Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5aa8315f-3d37-409f-98cd-ebaa091b91fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4: Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f08e42ae-9ecd-4e45-857c-5e36b1b20712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3eb03fee-81f4-4d22-8fb7-62d52a2bd2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read your parquet files from part 3\n",
    "df = spark.read.format(\"parquet\").load(\"hdfs://nn:9000/Rain_Forecast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d927717b-0739-4829-8578-c609d772a1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "574643bc-2225-4da4-998a-f6d44c1b5e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the VectorAssembler to combine the feature columns \n",
    "#(\"month\", \"sub1degrees\", \"sub1raining\", \"sub2degrees\", \"sub2raining\") into a single column.\n",
    "va = VectorAssembler(inputCols=[\"month\", \"sub1degrees\", \"sub1raining\", \"sub2degrees\", \"sub2raining\"], outputCol=\"features\")\n",
    "data = va.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e738522-34fe-487e-8e97-bc01f3f26731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split your data into train and test.\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e12182ec-921d-4af1-af10-68e6a1515f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a DecisionTreeClassifier on your training data to predict raining based on the features.\n",
    "dt_classifier = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"raining\", predictionCol=\"prediction\", maxDepth=5)\n",
    "dt_model = dt_classifier.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "391acf53-dd3e-4caa-8a66-0c6c2e7d886d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_079b4c3caa21, depth=5, numNodes=33, numClasses=2, numFeatures=5\n",
      "  If (feature 2 <= 0.5)\n",
      "   If (feature 0 <= 3.5)\n",
      "    If (feature 0 <= 2.5)\n",
      "     If (feature 3 <= 62.97582244873047)\n",
      "      Predict: 0.0\n",
      "     Else (feature 3 > 62.97582244873047)\n",
      "      Predict: 1.0\n",
      "    Else (feature 0 > 2.5)\n",
      "     Predict: 0.0\n",
      "   Else (feature 0 > 3.5)\n",
      "    If (feature 0 <= 5.5)\n",
      "     If (feature 3 <= 36.147695541381836)\n",
      "      If (feature 3 <= 25.290410041809082)\n",
      "       Predict: 1.0\n",
      "      Else (feature 3 > 25.290410041809082)\n",
      "       Predict: 0.0\n",
      "     Else (feature 3 > 36.147695541381836)\n",
      "      If (feature 1 <= 52.76169204711914)\n",
      "       Predict: 1.0\n",
      "      Else (feature 1 > 52.76169204711914)\n",
      "       Predict: 0.0\n",
      "    Else (feature 0 > 5.5)\n",
      "     Predict: 0.0\n",
      "  Else (feature 2 > 0.5)\n",
      "   If (feature 3 <= 31.003357887268066)\n",
      "    If (feature 0 <= 3.5)\n",
      "     If (feature 0 <= 1.5)\n",
      "      If (feature 1 <= 33.99040985107422)\n",
      "       Predict: 1.0\n",
      "      Else (feature 1 > 33.99040985107422)\n",
      "       Predict: 0.0\n",
      "     Else (feature 0 > 1.5)\n",
      "      Predict: 0.0\n",
      "    Else (feature 0 > 3.5)\n",
      "     If (feature 3 <= 28.538591384887695)\n",
      "      Predict: 1.0\n",
      "     Else (feature 3 > 28.538591384887695)\n",
      "      Predict: 0.0\n",
      "   Else (feature 3 > 31.003357887268066)\n",
      "    If (feature 1 <= 71.47354888916016)\n",
      "     Predict: 1.0\n",
      "    Else (feature 1 > 71.47354888916016)\n",
      "     If (feature 3 <= 71.53899002075195)\n",
      "      If (feature 1 <= 75.75273132324219)\n",
      "       Predict: 0.0\n",
      "      Else (feature 1 > 75.75273132324219)\n",
      "       Predict: 1.0\n",
      "     Else (feature 3 > 71.53899002075195)\n",
      "      Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dt_model.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18003193-0c12-4309-9c4b-360f20728f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the model to make predictions on the test data.\n",
    "predictions = dt_model.transform(test_data)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"raining\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8607fdb9-6dcf-4371-af02-303f40653936",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_raining = predictions.filter(predictions.raining == 1).count() / predictions.select(\"raining\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40a19d84-68d7-46b4-bff7-3c17834736c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|      avg(correct)|       avg(raining)|\n",
      "+------------------+-------------------+\n",
      "|0.7595541401273885|0.3471337579617834|\n",
      "+------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "#What is the accuracy (percent of times the model is correct)?\n",
    "print(\"+------------------+-------------------+\")\n",
    "print(\"|      avg(correct)|       avg(raining)|\")\n",
    "print(\"+------------------+-------------------+\")\n",
    "print(f\"|{accuracy}|{avg_raining}|\")\n",
    "print(\"+------------------+-------------------+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad363391-d62c-4944-b7fb-a1eb43f4ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcee68c2-418d-447a-9761-032501731cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"station STRING, date DATE, raining INTEGER, month INTEGER, sub1degrees FLOAT, sub1raining INTEGER, sub2degrees FLOAT, sub2raining INTEGER\"\n",
    "readstream_df = spark.readStream.schema(schema).option(\"maxFilesPerTrigger\", 1).parquet(\"hdfs://nn:9000/Rain_Forecast\")\n",
    "\n",
    "data = va.transform(readstream_df)\n",
    "predictions = dt_model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6803b228-f348-4cce-907d-4f5458f2b850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/28 21:05:24 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-5c2dde42-81e9-4e3f-8e77-06e08f8a09d9. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/28 21:05:24 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2000-01-10|       0.0|\n",
      "|      A|2000-01-23|       0.0|\n",
      "|      A|2000-01-26|       0.0|\n",
      "|      A|2000-02-29|       0.0|\n",
      "|      A|2000-03-12|       1.0|\n",
      "|      A|2000-03-18|       0.0|\n",
      "|      A|2000-03-21|       0.0|\n",
      "|      A|2000-05-15|       1.0|\n",
      "|      A|2000-05-16|       1.0|\n",
      "|      A|2000-05-20|       0.0|\n",
      "|      A|2000-05-22|       1.0|\n",
      "|      A|2000-05-29|       0.0|\n",
      "|      A|2000-06-16|       1.0|\n",
      "|      A|2000-06-24|       1.0|\n",
      "|      A|2000-06-27|       1.0|\n",
      "|      A|2000-01-08|       0.0|\n",
      "|      A|2000-02-18|       0.0|\n",
      "|      A|2000-03-08|       1.0|\n",
      "|      A|2000-03-14|       1.0|\n",
      "|      A|2000-03-24|       0.0|\n",
      "+-------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2000-07-09|       0.0|\n",
      "|      A|2000-07-25|       1.0|\n",
      "|      A|2000-08-03|       0.0|\n",
      "|      A|2000-08-05|       0.0|\n",
      "|      A|2000-07-04|       1.0|\n",
      "|      A|2000-07-18|       0.0|\n",
      "|      A|2000-07-22|       1.0|\n",
      "|      A|2000-07-29|       0.0|\n",
      "|      A|2000-06-30|       0.0|\n",
      "|      A|2000-07-11|       0.0|\n",
      "|      A|2000-07-12|       1.0|\n",
      "|      A|2000-07-14|       0.0|\n",
      "|      A|2000-07-19|       0.0|\n",
      "|      A|2000-07-23|       1.0|\n",
      "|      A|2000-07-24|       1.0|\n",
      "|      A|2000-07-26|       1.0|\n",
      "|      A|2000-08-09|       1.0|\n",
      "|      A|2000-07-05|       1.0|\n",
      "|      A|2000-07-06|       1.0|\n",
      "|      A|2000-07-21|       0.0|\n",
      "+-------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2000-09-11|       1.0|\n",
      "|      A|2000-09-16|       1.0|\n",
      "|      A|2000-09-30|       0.0|\n",
      "|      A|2000-10-07|       1.0|\n",
      "|      A|2000-08-13|       1.0|\n",
      "|      A|2000-08-19|       1.0|\n",
      "|      A|2000-08-20|       1.0|\n",
      "|      A|2000-08-21|       1.0|\n",
      "|      A|2000-08-26|       0.0|\n",
      "|      A|2000-08-30|       0.0|\n",
      "|      A|2000-09-23|       1.0|\n",
      "|      A|2000-09-29|       0.0|\n",
      "|      A|2000-08-12|       1.0|\n",
      "|      A|2000-08-16|       1.0|\n",
      "|      A|2000-08-27|       0.0|\n",
      "|      A|2000-09-15|       1.0|\n",
      "|      A|2000-10-04|       0.0|\n",
      "|      A|2000-08-15|       1.0|\n",
      "|      A|2000-09-13|       1.0|\n",
      "|      A|2000-09-22|       0.0|\n",
      "+-------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Use features as a source and apply your model to stream weather predictions to the screen\n",
    "df = predictions.select(\"station\", \"date\", \"prediction\").filter(col(\"station\") == \"A\")\n",
    "s = df.writeStream.format(\"console\").trigger(processingTime=\"5 seconds\").outputMode(\"append\").start()\n",
    "s.awaitTermination(20)\n",
    "s.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8d75ef-52e4-4e1c-b7c6-98d0c5ea8dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3213960-537d-4cbb-afcd-417241c00fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, time, random, string, json, threading\n",
    "from produce_pb2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83fb15f8-0139-4298-b587-216e902e21f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_station(name):\n",
    "    # temp pattern\n",
    "    month_avg = [27,31,44,58,70,79,83,81,74,61,46,32]\n",
    "    shift = (random.random()-0.5) * 30\n",
    "    month_avg = [m + shift + (random.random()-0.5) * 5 for m in month_avg]\n",
    "    \n",
    "    # rain pattern\n",
    "    start_rain = [0.1,0.1,0.3,0.5,0.4,0.2,0.2,0.1,0.2,0.2,0.2,0.1]\n",
    "    shift = (random.random()-0.5) * 0.1\n",
    "    start_rain = [r + shift + (random.random() - 0.5) * 0.2 for r in start_rain]\n",
    "    stop_rain = 0.2 + random.random() * 0.2\n",
    "\n",
    "    # day's state\n",
    "    today = datetime.date(2000, 1, 1)\n",
    "    temp = month_avg[0]\n",
    "    raining = False\n",
    "    \n",
    "    # gen weather\n",
    "    while True:\n",
    "        # choose temp+rain\n",
    "        month = today.month - 1\n",
    "        temp = temp * 0.8 + month_avg[month] * 0.2 + (random.random()-0.5) * 20\n",
    "        if temp < 32:\n",
    "            raining=False\n",
    "        elif raining and random.random() < stop_rain:\n",
    "            raining = False\n",
    "        elif not raining and random.random() < start_rain[month]:\n",
    "            raining = True\n",
    "\n",
    "        yield (today.strftime(\"%Y-%m-%d\"), name, temp, raining)\n",
    "\n",
    "        # next day\n",
    "        today += datetime.timedelta(days=1)\n",
    "        \n",
    "def all_stations(count=10, sleep_sec=1):\n",
    "    assert count <= 26\n",
    "    stations = []\n",
    "    for name in string.ascii_uppercase[:count]:\n",
    "        stations.append(one_station(name))\n",
    "    while True:\n",
    "        for station in stations:\n",
    "            yield next(station)\n",
    "        time.sleep(sleep_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af4aa3e-d59c-4248-a592-fdf4d5853f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops forever because the weather never ends...\n",
    "# for row in all_stations(3):\n",
    "#     print(row) # date, station, temp, raining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8a6f861-46a9-4f10-b55e-969ef8bee34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['stations-json', 'stations']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka import KafkaAdminClient, KafkaProducer, KafkaConsumer, TopicPartition\n",
    "from kafka.admin import NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError, UnknownTopicOrPartitionError\n",
    "\n",
    "admin = KafkaAdminClient(bootstrap_servers=[\"kafka:9092\"])\n",
    "try:\n",
    "    admin.delete_topics([\"stations\", \"stations-json\"])\n",
    "    print(\"deleted\")\n",
    "except UnknownTopicOrPartitionError:\n",
    "    print(\"cannot delete (may not exist yet)\")\n",
    "\n",
    "time.sleep(1)\n",
    "admin.create_topics([NewTopic(\"stations\", 6, 1)])\n",
    "admin.create_topics([NewTopic(\"stations-json\", 6, 1)])\n",
    "admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d1b7582-366c-4c37-8ea2-3a275dd593c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def produce():\n",
    "    producer = KafkaProducer(bootstrap_servers=[\"kafka:9092\"], acks=\"all\", retries=10)\n",
    "    \n",
    "    for date, station, degrees, raining in all_stations(15):\n",
    "        # send to \"stations\" stream using protobuf\n",
    "        resp = Report(date=str(date), station=str(station), degrees=degrees, raining=bool(raining))\n",
    "        producer.send(\"stations\", key=bytes(station, \"utf-8\"), value=resp.SerializeToString())\n",
    "        # send to \"stations-json\" using JSON\n",
    "        rain = 0\n",
    "        if raining:\n",
    "            rain = 1\n",
    "        d = {\"date\":date, \"station\":station,  \"degrees\":degrees, \"raining\":rain}\n",
    "        producer.send(\"stations-json\", key=bytes(station, \"utf-8\"), value=bytes(json.dumps(d), \"utf-8\"))\n",
    "# start thread to run produce\n",
    "t1 = threading.Thread(target=produce)\n",
    "t1.start()\n",
    "\n",
    "# never join thread because we want it to run forever\n",
    "\n",
    "#check if thread ran successfully\n",
    "# while True:\n",
    "#     time.sleep(1)\n",
    "#     if t1.is_alive():\n",
    "#         print('Still running')\n",
    "#     else:\n",
    "#         print('Completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "420a8be6-4fed-4fe9-b9f2-b3ca4d1985a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: Kafka Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "544f9ba0-06b7-46d1-901c-be12293e7595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "for partition in range(6):\n",
    "    path = f\"partition-{partition}.json\"\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "065b0ff8-5911-4b33-93c9-16761905f6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_partition(partition_num):\n",
    "    path = f\"partition-{partition_num}.json\"\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\") as file:\n",
    "            return json.load(file)\n",
    "    else:\n",
    "        return { \"offset\": 0, \"partition\": partition_num }\n",
    "\n",
    "def save_partition(partition):\n",
    "    path = f\"partition-{partition['partition']}.json\"\n",
    "    path2 =  path + \".tmp\"\n",
    "    with open(path2, \"w\") as file:\n",
    "        json.dump(partition, file)\n",
    "        os.rename(path2, path)\n",
    "        \n",
    "#atomic file writes:\n",
    "# path = ????\n",
    "# path2 = path + \".tmp\"\n",
    "# with open(path2, \"w\") as f:\n",
    "#     # TODO: write the data\n",
    "#     os.rename(path2, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c3a4e3d-4c06-458c-9440-45cefefd9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updatePartition(partition, resp_dict_):\n",
    "    lock = threading.Lock()\n",
    "    with lock:\n",
    "        if resp_dict_[\"station\"] not in partition.keys():\n",
    "            partition[resp_dict_[\"station\"]] = {}\n",
    "            \n",
    "        if \"end\" not in partition[resp_dict_[\"station\"]].keys():\n",
    "            partition[resp_dict_[\"station\"]][\"end\"] = resp_dict_[\"date\"]\n",
    "            partition[resp_dict_[\"station\"]][\"start\"] = resp_dict_[\"date\"]\n",
    "        else:\n",
    "            d1 = partition[resp_dict_[\"station\"]][\"end\"].split('-')\n",
    "            d1 = datetime.datetime(int(d1[0]), int(d1[1]), int(d1[2]))\n",
    "            d2 = resp_dict_[\"date\"].split('-')\n",
    "            d2 = datetime.datetime(int(d2[0]), int(d2[1]), int(d2[2]))\n",
    "            if d2 > d1:\n",
    "                partition[resp_dict_[\"station\"]][\"end\"] = resp_dict_[\"date\"]\n",
    "            else:\n",
    "                return partition\n",
    "\n",
    "        if \"sum\" not in partition[resp_dict_[\"station\"]].keys():\n",
    "            partition[resp_dict_[\"station\"]][\"sum\"] = float(resp_dict_[\"degrees\"])\n",
    "        else:\n",
    "            partition[resp_dict_[\"station\"]][\"sum\"] += float(resp_dict_[\"degrees\"])\n",
    "\n",
    "        if \"count\" not in partition[resp_dict_[\"station\"]].keys():\n",
    "            partition[resp_dict_[\"station\"]][\"count\"] = 1\n",
    "        else:\n",
    "            partition[resp_dict_[\"station\"]][\"count\"] += 1\n",
    "\n",
    "        if \"avg\" not in partition[resp_dict_[\"station\"]].keys():\n",
    "            partition[resp_dict_[\"station\"]][\"avg\"] = float(resp_dict_[\"degrees\"])\n",
    "        else:\n",
    "            partition[resp_dict_[\"station\"]][\"avg\"] = partition[resp_dict_[\"station\"]][\"sum\"] / partition[resp_dict_[\"station\"]][\"count\"]\n",
    "\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c7b7bd4-8563-4610-9e77-82a2aa82f818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND 0\n",
      "exiting\n",
      "exiting\n",
      "exiting\n",
      "ROUND 1\n",
      "exiting\n",
      "exitingexiting\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def consume(part_nums=[], iterations=10):\n",
    "    consumer = KafkaConsumer(bootstrap_servers=[\"kafka:9092\"])\n",
    "    # create list of TopicPartition objects\n",
    "    topics = [TopicPartition(\"stations\", part_nums[0]), TopicPartition(\"stations\", part_nums[1])]\n",
    "    consumer.assign(topics)\n",
    "    # PART 1: initialization\n",
    "    partitions = {} # key=partition num, value=snapshot dict\n",
    "    # load partitions from JSON files (if they exist) or create fresh dicts\n",
    "    partitions[part_nums[0]] = load_partition(part_nums[0])\n",
    "    partitions[part_nums[1]] = load_partition(part_nums[1])\n",
    "    # if offsets were specified in previous JSON files, the consumer\n",
    "    # should seek to those; else, seek to offset 0.\n",
    "    for partition in partitions:\n",
    "        consumer.seek(TopicPartition(\"stations\", partition), partitions[partition][\"offset\"])\n",
    "    # PART 2: process batches\n",
    "    for i in range(iterations):\n",
    "        batch = consumer.poll(1000) # 1s timeout\n",
    "        for topic, messages in batch.items():\n",
    "            for msg in messages:\n",
    "            # update the partitions based on new messages\n",
    "            # Report.FromString(msg.value), report_pb2.Report().ParseFromString(msg.value)\n",
    "                resp = str(Report.FromString(msg.value)).replace('\\\"', ' ').split('\\n')\n",
    "                resp_dict_ = {}\n",
    "                for data in resp:\n",
    "                    kv_pair = data.split(':')\n",
    "                    resp_dict_[kv_pair[0]] = kv_pair[-1]\n",
    "                partitions[msg.partition] = updatePartition(partitions[msg.partition], resp_dict_)\n",
    "                for tp in consumer.assignment():\n",
    "                    partitions[tp.partition][\"offset\"] = consumer.position(tp)\n",
    "                # save the data back to the JSON file\n",
    "                for partition in partitions:\n",
    "                    save_partition(partitions[partition])\n",
    "    print(\"exiting\")\n",
    "\n",
    "for i in range(2):\n",
    "    print(\"ROUND\", i)\n",
    "    t1 = threading.Thread(target=consume, args=([0,1], 30))\n",
    "    t2 = threading.Thread(target=consume, args=([2,3], 30))\n",
    "    t3 = threading.Thread(target=consume, args=([4,5], 30))\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t3.start()\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    t3.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "351990f8-9984-4ee0-98f4-cdd068104014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"offset\": 51, \"partition\": 0, \"  N \": {\"end\": \"  2000-02-20 \", \"start\": \"  2000-01-01 \", \"sum\": 2003.7745612544854, \"count\": 51, \"avg\": 39.28969727949971}}{\"offset\": 102, \"partition\": 1, \"  E \": {\"end\": \"  2000-02-20 \", \"start\": \"  2000-01-01 \", \"sum\": 1007.2312302797652, \"count\": 51, \"avg\": 19.749631966269906}, \"  O \": {\"end\": \"  2000-02-20 \", \"start\": \"  2000-01-01 \", \"sum\": 2063.0232002616667, \"count\": 51, \"avg\": 40.45143529924837}}{\"offset\": 156, \"partition\": 2, \"  F \": {\"end\": \"  2000-02-21 \", \"start\": \"  2000-01-01 \", \"sum\": 1838.5927544225622, \"count\": 52, \"avg\": 35.35755296966466}, \"  I \": {\"end\": \"  2000-02-21 \", \"start\": \"  2000-01-01 \", \"sum\": 1910.0227400261424, \"count\": 52, \"avg\": 36.73120653896428}, \"  J \": {\"end\": \"  2000-02-21 \", \"start\": \"  2000-01-01 \", \"sum\": 1485.9166655019587, \"count\": 52, \"avg\": 28.575320490422282}}{\"offset\": 156, \"partition\": 3, \"  D \": {\"end\": \"  2000-02-21 \", \"start\": \"  2000-01-01 \", \"sum\": 549.7057402295935, \"count\": 52, \"avg\": 10.57126423518449}, \"  G \": {\"end\": \"  2000-02-21 \", \"start\": \"  2000-01-01 \", \"sum\": 901.5959398349892, \"count\": 52, \"avg\": 17.338383458365175}, \"  M \": {\"end\": \"  2000-02-21 \", \"start\": \"  2000-01-01 \", \"sum\": 2447.1153375663657, \"count\": 52, \"avg\": 47.05991033781473}}{\"offset\": 260, \"partition\": 4, \"  A \": {\"end\": \"  2000-02-21 \", \"start\": \"  2000-01-01 \", \"sum\": 1096.1349694548398, \"count\": 52, \"avg\": 21.079518643362306}, \"  B \": {\"end\": \"  2000-02-21 \", \"start\": \"  2000-01-01 \", \"sum\": 1898.3038895066552, \"count\": 52, \"avg\": 36.50584402897414}, \"  C \": {\"end\": \"  2000-02-21 \", \"start\": \"  2000-01-01 \", \"sum\": 1825.0613734794379, \"count\": 52, \"avg\": 35.097334105373804}, \"  K \": {\"end\": \"  2000-02-21 \", \"start\": \"  2000-01-01 \", \"sum\": 1232.7532314996263, \"count\": 52, \"avg\": 23.70679291345435}, \"  L \": {\"end\": \"  2000-02-21 \", \"start\": \"  2000-01-01 \", \"sum\": 819.0364712928222, \"count\": 52, \"avg\": 15.750701371015811}}{\"offset\": 52, \"partition\": 5, \"  H \": {\"end\": \"  2000-02-21 \", \"start\": \"  2000-01-01 \", \"sum\": 1741.883103585606, \"count\": 52, \"avg\": 33.497751992030885}}"
     ]
    }
   ],
   "source": [
    "!cat partition*.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
